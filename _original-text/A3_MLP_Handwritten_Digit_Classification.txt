A3: 3層型ニューラルネットワークを用いた数字認識

================
1. はじめに

ここでは、Google社が開発した機械学習用のプラットフォーム TensorFlow を用いて、数字画像認識を行うプログラムを作成する。作成には、Web上でPythonを記述・実行することができるGoogle Colaboratory を用いる。Webの教材ページに、サンプルプログラムをノートブック形式で掲載するので、そのプログラムを実際に実行しながら、動作確認をすると良い。なお、プログラムを完全に理解するには、PythonやNumpy（Python用の数値計算ライブラリ）の使い方を知る必要があるので、必要に応じて学習されたい。

https://www.tensorflow.org/tutorials?hl=ja
https://colab.research.google.com/notebooks/intro.ipynb?hl=ja

================
2. 画像データと対応する教師データの読み込み

最初にTensorFlowとNumpy のモジュールをインポートし、数字の画像データベースであるMNISTを読み込む。

```
import tensorflow as tf  #TensorFlowモジュールのインポート
import numpy as np  #NumPyモジュールのインポート
mnist = tf.keras.datasets.mnist  #数字画像データベースの参照変数の設定
(x_train, y_train), (x_test, y_test) = mnist.load_data()  #画像の読み込み
print(type(x_train))  #x_trainのデータ型の確認
print(x_train.dtype)  #x_train(多次元配列)の各要素のデータ型の確認
print(x_train.ndim)  #x_train配列の次元数の確認
print(x_train.shape)  #x_train配列の大きさの確認
```

ここで、x_train, y_train, x_test, y_testのデータ型はNumPyの多次元配列(numpy.ndarray)となる。x_trainは学習用データで数字画像を表現している3次元配列である。各文字画像は28×28画素であり、各画素は0から255の間の整数(uint8)で表現され（グレースケール画像）、このような文字画像が60,000枚、含まれている。一方、y_trainは学習用データの教師データを示している1次元配列である。各要素は0から9の間の整数値(uint8)で表現され、全体で60,000個が含まれている。x_test, y_testはテスト用データの文字画像とその教師データをそれぞれ示しており、データ構造は、x_trainとy_trainと同様である。ただし、文字画像と教師データの個数はそれぞれ10,000個となっている。（上記プログラムのように多次元配列の各属性値はprint文で確認可能）。

```
x_train, x_test = x_train / 255.0, x_test / 255.0
```

次に、上記の計算をすることにより、x_trainとx_testの各画素値を0.0から1.0の浮動小数点数(float64)に変換する。このように、ニューラルネットワークへの入力値は0から1に正規化することが多い。以下、x_train, y_trainの学習データを図示すると次のような構成になっている。なお、x_train[i] (x_trainのi番目のデータ)の画像に対する教師データ(正解データ)はy_train[i]に格納されている。


図33　学習データの構成


================
3. ニューラルネットワークの構成と学習法の設定

次に、ニューラルネットワークの構成と学習法を設定する。ここでは、次のように設定してみる。

*  入力層へは、28画素×28画素を1列に並べた784次元のベクトル（各画素は0～1の値）を入力する。よって、入力層のユニット数は784個となる。
*  中間層を64個とし、活性化関数はシグモイド関数を採用。
*  出力層は、数字のクラス数10個に合わせて、ユニット数は10個とし、ソフトマックス関数で各クラスの確率値を出力。
*  最適化アルゴリズムとしてSGD（確率的勾配降下法）を採用。
*  誤差関数としてクラス出力に関するクロスエントロピーを採用
*  学習途中の評価のための尺度として識別精度(accuracy)を指定

以上を実現するために次のように記述する。

```
model = tf.keras.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(64, activation='sigmoid'), 
  tf.keras.layers.Dense(10)
])
model.compile(optimizer='sgd',
              loss=tf.keras.losses.SparseCategoricalCrossentropy
(from_logits=True),
              metrics=['accuracy'])
```

上記の構造を図示すると次のとおりである。


図34　ニューラルネットワークの構成


================
4. ニューラルネットワークの学習と評価

次に、学習用データを用いて構成したニューラルネットワークを使って学習を行う。ここでは、学習用データとして、先ほど読み込んだ x_train と y_trainのデータを用いる。x_trainは60,000枚の数字画像（グレースケール画像）であり、y_trainはそれらの各画像に対応する正解データ(0から9の数字)である。学習を実施するには、下記プログラムを実行する。

```
model.fit(x_train, y_train, epochs=5)
```

ここで、エポック(epoch)とは、数字画像全体(60,000枚)を一通り学習すると1回とカウントする数値なので、この場合は5回繰り返し学習していることになる。学習結果は次のとおりである。なお、学習時のネットワークの重みの初期値が乱数によって設定されるため、学習ごとに微妙に結果の値が異なる点を留意されたい。


これより、学習データについては、89.03%の認識率が得られていることがわかる。次に、テストデータについての認識率を評価してみよう。

```
test_loss, test_acc = model.evaluate(x_test, y_test)
print(test_acc)
```


この結果から、テストデータの認識率は90.04%となっていることがわかる。


================
5. ニューラルネットワークの構成・学習法の変更による効果

次に、学習に用いるニューラルネットワークの構造や各種パラメータを変更して、さらにテストデータの認識率を向上させることができるかを検証してみよう。

最初に中間層の活性化関数(*1) をシグモイド関数からReLUに変更してみる。プログラムで、sigmoidの部分をreluに変更する（下記 activation='relu' の部分）。

> (*1) 利用可能な活性化関数は次のURLを参照：https://keras.io/ja/activations/

```
model = tf.keras.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(64, activation='relu'), 
  tf.keras.layers.Dense(10)
])
```

他のプログラムは前回と同様である。このモデルを用いた結果、学習データに対する認識率は93.04%、テストデータに対する認識率は93.36%に向上する。

次に、中間層のユニット数を64個から128個に増やしてみる。プログラムで、下記 tf.keras.layers.Dense(128 の部分を修正する。

```
model = tf.keras.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(128, activation='relu'), 
  tf.keras.layers.Dense(10)
])
```

結果、学習データとテストデータに対する認識率は、それぞれ93.47%、93.81%と若干ではあるが向上がみられた。その後、さらにユニット数を256個とする実験も行ったが、テストデータに対する認識率は、ほとんど変わらなかったため、ここでは128個に固定する。

次に、最適化アルゴリズム(*2) をSGDからAdamに変更してみる。プログラムでは、下記 optimizer='adam' の部分を修正する。

> (*2) 利用可能な最適化アルゴリズムは次のURLを参照：https://keras.io/ja/optimizers/

```
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy
(from_logits=True),
              metrics=['accuracy'])
```

この結果、学習データとテストデータに対する認識率は、それぞれ 98.62%、97.74%と向上した（以後、Adamに固定）。

次に、中間層の数を1つ増やして、全体で4層構造のニューラルネットワークにしてみる。ここでは、中間層の第1層は128個のままとし、第2層として64個のものを追加する。使用する活性化関数はともにReLUである。

```
model = tf.keras.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dense(64, activation='relu'),
  tf.keras.layers.Dense(10)
])
```

この結果、学習データとテストデータに対する認識率は、それぞれ 98.68%、97.56%となった。これより、学習データについては若干向上したが、テストデータについては若干下がってしまった。この傾向は、学習データに過剰に適応した過学習（overfitting）の状態を示しているため、以後は、中間層を1層（64個）として固定することにする。

次に、学習回数（エポック）を5から10に変更してみよう。

```
model.fit(x_train, y_train, epochs=10)
```

結果、学習データとテストデータに対する認識率は、それぞれ 99.51%、97.78%となり、ここでも、過学習傾向となったため、エポック数は5程度で十分と考えられる。

最後に、過学習の傾向を抑制するために、ドロップアウトの機能を導入してみよう。ここでは、出力層以外のユニットのうち、2割をランダムに無効化して学習することにする。

```
model = tf.keras.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(128, activation='relu'), 
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10)
])
```

これにより、学習データとテストデータに対する認識率は、それぞれ 97.69%、97.85%となり、学習データについては認識率は落ちるが（過学習傾向の抑制）、テストデータについての認識率は向上していることがわかる。同じ条件で、ドロップアウトの確率を4割にした場合は、学習・テストデータ両方で、若干、認識率が下がってしまった。これより、あまり多くのユニットを削減してしまうと、認識率の低下につながってしまうため、適切な値を見つける必要がある。

以上、今回の構造やパラメータ変更によって得られた結果を下表にまとめる。このように、ニューラルネットワークの学習を行う際には、様々なパラメータを試して、最適なモデルを探る必要がある。



表 9　各種パラメータを変更した場合の認識率


（以上）
========================================================================

